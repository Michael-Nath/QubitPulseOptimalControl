{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from Env import Env\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU or CPU depending on device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition tupple to define a lived experience\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"next_state\", \"action\", \"reward\"))\n",
    "# Experience Replay Object\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        new_transition = Transition(*args)\n",
    "        self.buffer.append(new_transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.buffer, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture (Work In Progress)\n",
    "class QubitQNetwork(nn.Module):\n",
    "    def __init__(self, n_time_slots, n_actions, n_features):\n",
    "        super(QubitQNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(n_features, n_time_slots)\n",
    "        self.l2 = nn.Linear(n_time_slots, n_actions)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightConstraint:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            w = w.clamp(-1, 1)\n",
    "            module.weight.data = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "BATCH_SIZE = 1\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 4\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "env = Env(N=5)\n",
    "n_actions = env.n_actions #UNCOMMENT ONLY AFTER ENVIRONMENT IS READY TO BE USED\n",
    "n_features = env.n_features\n",
    "n_time_slots = n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing neural nets\n",
    "weight_constraint = WeightConstraint()\n",
    "policy_net = QubitQNetwork(n_time_slots, n_actions, n_features).to(device) # network that gets fitted every iteration (\"training\" net for target_net)\n",
    "policy_net._modules['l2'].apply(weight_constraint)\n",
    "# policy_net = policy_net.float()\n",
    "target_net = QubitQNetwork(n_time_slots, n_actions, n_features).to(device) # network that predicts (gets fitted every n iterations) and calculates loss / optimizes for policy_net\n",
    "# target_net._modules['l2'].apply(weight_constraint)\n",
    "# target_net = target_net.float()\n",
    "target_net.load_state_dict(policy_net.state_dict()) # copies initial Ws and Bs from policy_net to target_net\n",
    "target_net.eval()\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done # ensures that as our algo progresses, we are choosing less and less random actions\n",
    "    sample = random.random()\n",
    "    steps_done += 1\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            res = policy_net(state.float())\n",
    "            return res.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    # policy_net._modules['l2'].apply(weight_constraint)\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    state_action_values = policy_net(state_batch.float()).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states.float()).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        a = list(policy_net.parameters())[2].clone()\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()\n",
    "        b = list(policy_net.parameters())[2].clone()\n",
    "        # print(torch.mean(a), torch.mean(b))\n",
    "        # print(torch.equal(a.data, b.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2706, grad_fn=<MeanBackward0>) 4\n",
      "tensor(0.2688, grad_fn=<MeanBackward0>) 2\n",
      "tensor(0.2750, grad_fn=<MeanBackward0>) 1\n",
      "tensor(0.2767, grad_fn=<MeanBackward0>) 3\n",
      "tensor(0.2740, grad_fn=<MeanBackward0>) 4\n",
      "DONE\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_episodes = 1\n",
    "for ith_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    state = torch.reshape(state, (1,4))\n",
    "    while steps_done < n_time_slots:\n",
    "        action = select_action(state)\n",
    "        coefficient = torch.mean(target_net.l2.weight[action.item()]).detach().numpy()\n",
    "        print(torch.mean(policy_net.l2.weight), action.item())\n",
    "        new_state, reward, done, fid = env.step(action.item, coefficient)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if not done:\n",
    "            new_state = torch.from_numpy(new_state)\n",
    "            new_state = torch.reshape(new_state, (1,4))\n",
    "        else:\n",
    "            new_state = None\n",
    "        memory.push(state, new_state, action, reward)\n",
    "        state = new_state\n",
    "        optimize_model()\n",
    "        # print(fid)\n",
    "        if done:\n",
    "            print(fid)\n",
    "            break\n",
    "    steps_done = 0\n",
    "    if ith_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    print(\"DONE\")\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
